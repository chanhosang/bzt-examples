# https://gettaurus.org/docs/ConfigSyntax/
# optional arguments:
# -o modules.jmeter.properties="{'jmeter.reportgenerator.overall_granularity':60000}"
# -o modules.jmeter.properties="{'jmeter.reportgenerator.report_title': Load Test Dashboard}"


---
settings:
  env:
    APP_HOST: test-api.k6.io
    JMX_1: jmeter-test-1
    JMX_2: jmeter-test-2

execution:

# Test Scenarios
# https://gettaurus.org/docs/JMeter/#Run-Existing-JMX-File
# https://marketplace.visualstudio.com/items?itemName=AlexandreGattiker.jmeter-tasks
- executor: jmeter
  scenario: SCENARIO-1
  concurrency: 1
  ramp-up: 1s
  iterations: 1
  # hold-for: 1m

- executor: jmeter
  scenario: SCENARIO-2
  concurrency: 1
  ramp-up: 1s
#   iterations: 1
  hold-for: 1m


scenarios:
  SCENARIO-1:
    variables:
      HOST: ${APP_HOST}
    script: jmeter/src/test/jmeter/performance/${JMX_1}.jmx

  SCENARIO-2:
    variables:
      HOST: ${APP_HOST}
    script: jmeter/src/test/jmeter/performance/${JMX_2}.jmx


modules:
  # A flag to run scenarios in parallel or sequential
  local:
    sequential: false  # false by default
  jmeter:
    csv-jtl-flags:
      saveAssertionResultsFailureMessage: true
      sentBytes: true
    ## To override from command line
    # properties:
    #   jmeter.reportgenerator.apdex_satisfied_threshold: 1000
    #   jmeter.reportgenerator.report_title: JMeter Taurus Demo
    #   jmeter.reportgenerator.overall_granularity: 1000

  ## If you have API Key, just export it as environment variable and uncomment the 'token'
  ## If No, it will upload results anonymously to BlazeMeter Dashboard
  blazemeter:
    project: Demo
    report-name: Demo Load Test Results
    # token: <api-key-id>:<api-key-secret>

services:
- module: shellexec
  prepare:
  - mkdir "${TAURUS_ARTIFACTS_DIR}/results"
